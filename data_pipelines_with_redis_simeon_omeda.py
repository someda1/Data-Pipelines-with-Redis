# -*- coding: utf-8 -*-
"""Data Pipelines with Redis-simeon omeda.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OjP_Y3ZhEhHoEkZZv7rgkitGEK4qXNGk

### **Data Pipelines with Redis**

**Background Information**

You have been hired by a telecommunications company that wants to optimize their business
processes. They have a Neo4j graph database that contains information about their customers,
their subscriptions, and the services they are using. However, they also want to store this data
in a more traditional relational database to allow for easier querying and analysis. They have
asked you to create a data pipeline that extracts data from their Neo4j database, transforms it
using pandas, and loads it into a Postgres database.

**Guidelines**

***1. Extracting data from Neo4j:***
To extract data from the Neo4j database, you will need to use the Neo4j Python driver.
You must authenticate with the database and write a Cypher query to extract the
necessary data. The fields that you should extract from the database include the
following:

● Customer ID

● Subscription ID

● Service ID

● Start date of subscription

● End date of subscription

● Price of subscription

***2. Transforming data using Pandas:***

Once you have extracted the data from Neo4j, you must transform it using Pandas. You
should create a Pandas DataFrame from the extracted data and perform any necessary
data cleaning and manipulation. For example, you may need to convert date fields from
strings to datetime objects and remove null values.

***3. Loading data into Postgres:***

Finally, you must load the transformed data into a Postgres database. You should create
a new table in the Postgres database with the following fields:

● Customer ID (integer)

● Subscription ID (integer)

● Service ID (integer)

● Start date of subscription (date)

● End date of subscription (date)

● Price of subscription (float)

You should then use the psycopg2 Python library to connect to the Postgres database
and write a function to insert the transformed data into the new table.
You can find the file to start working on this project here [link].

***Deliverables***

We will be expected to deliver a GitHub repository with the following:

● A Python file containing extract, transform, and load functions.

● A README file explaining how to set up and run the data pipeline, explaining the data
schema and the transformations performed on the data.
"""

!pip install redis

#import the libraries needed
import pandas as pd
import psycopg2
from datetime import datetime
import logging
logging.basicConfig(filename='pipeline.log', level=logging.DEBUG)

import redis

r = redis.Redis(
  host='redis-18400.c305.ap-south-1-1.ec2.cloud.redislabs.com',
  port=18400,
  password='OW3AMcODuFDFWXQtKhehzyHiFlasAgI5')

# Postgres Database Information
pg_host = '35.196.132.175'
pg_database = 'someda_db'
pg_user = 'someda'
pg_password = '@someda123'

def extract_data():

    try:
        #read file from local drive
        data = pd.read_csv('customer_call_logs.csv')

        # Cache data in Redis for faster retrieval
        r.set('call_logs1', data.to_json(orient='records'))
    
    except Exception as e:
        err = "Extract() error - "+e
        logging.debug(err)
    
    return data

# Transform data
def transform_data():
    
    try:
        
        data = pd.read_json(r.get('call_logs1'))

        #Cast call_date column from object to datetime
        data['call_date'] = pd.to_datetime(data['call_date']) 

        transformed_data = data
    
    except Exception as e:
        err = "Transform() error - "+e
        logging.debug(err)
    
    return transformed_data

def load_data(transformed_data):

    try:
        # Connect to Postgres database
        conn = psycopg2.connect(host=pg_host, database=pg_database, user=pg_user, password=pg_password)

        # Create a cursor object
        cur = conn.cursor()

        # Create a table to store the data
        cur.execute('CREATE TABLE IF NOT EXISTS customer_call_logs (\
                     customer_id INT,\
                     call_cost_usd FLOAT,\
                     call_destination VARCHAR,\
                     call_date TIMESTAMP,\
                     call_duration_min VARCHAR\
                     )')

        # Insert the transformed data into the database
        for i, row in transformed_data.iterrows():
            cur.execute(f"INSERT INTO customer_call_logs (customer_id, call_cost_usd, call_destination, call_date, call_duration_min) VALUES ({row['customer_id']}, {row['call_cost']}, '{row['call_destination']}', '{row['call_date']}', '{row['call_duration']}')")

        # Commit the changes
        conn.commit()

        # Close the cursor and connection
        cur.close()
        conn.close()
    
    except Exception as e:
        err = "Load() error - "+e
        logging.debug(err)

def data_pipeline():
   
    extract_data()
    transformed_data = transform_data()
    load_data(transformed_data)

# Run the data pipeline function
if __name__ == '__main__':
 data_pipeline()